# LLM config file

# USD / 1M tokens
# Use field names referring to fields returned by the model API call (see official models docs)
[models."gpt-4.1-mini"]
cached_tokens = 0.10
prompt_tokens = 0.40
completion_tokens = 1.60

[models."gpt-4.1-nano"]
cached_tokens = 0.0025
prompt_tokens = 0.10
completion_tokens = 0.40

[models."deepseek/deepseek-chat-v3-0324"]
prompt_tokens = 0.25
completion_tokens = 0.88

[models."tngtech/deepseek-r1t2-chimera:free"]
prompt_tokens = 0
completion_tokens = 0

[models."qwen/qwen3-coder"]
prompt_tokens = 0.25
completion_tokens = 1

[models."gemini-2.5-flash-lite"]
prompt_tokens = 0.10
completion_tokens = 0.40

# EVALUATED TOPICS
[[topics]]
name = "Modularity"
weight = 1.0
description = "mod.md"

[[topics]]
name = "Correct use of dynamic memory"
weight = 1.0
description = "mem.md"

[[topics]]
name = "Appropriate data structures"
weight = 1.0
description = "data.md"

[[topics]]
name = "Error handling"
weight = 1.0
description = "errors.md"

# SUMMARY ANALYSIS
[[analysis]]
name = "priority issues"
description = "Mention the main problems identified in the analysis of the various topics, explaining where they occur and what they consist of."

[[analysis]]
name = "concrete suggestions"
description = "Based on the highlighted problems, provide concrete suggestions aimed at improving and correcting the program in the areas where such issues occur."
