# LLM config file

# USD / 1M tokens
[models."gpt-4.1-mini"]
input_tokens = 0.80    # OPENAI
cached_tokens = 0.20
output_tokens = 3.20   # OPENAI
prompt_tokens = 0.80
completion_tokens = 3.20

[models."gpt-4.1-nano"]
input_tokens = 0.20    # OPENAI
cached_tokens = 0.05
output_tokens = 0.80   # OPENAI
prompt_tokens = 0.20
completion_tokens = 0.80

[models."deepseek/deepseek-chat-v3-0324"]
prompt_tokens = 0.25
completion_tokens = 0.88

[models."qwen/qwen3-coder"]
prompt_tokens = 0.25
completion_tokens = 1

[models."gemini-2.5-flash-lite"]
prompt_token_count = 0.10
candidates_token_coun = 0.40

# EVALUATED TOPICS
[[topics]]
name = "Modularity"
weight = 1.0
description = "mod.md"

[[topics]]
name = "Correct use of dynamic memory"
weight = 1.0
description = "mem.md"

[[topics]]
name = "Appropriate data structures"
weight = 1.0
description = "data.md"

[[topics]]
name = "Error handling"
weight = 1.0
description = "errors.md"

# SUMMARY ANALYSIS
[[analysis]]
name = "priority issues"
description = "Mention the main problems identified in the analysis of the various topics, explaining where they occur and what they consist of."

[[analysis]]
name = "concrete suggestions"
description = "Based on the highlighted problems, provide concrete suggestions aimed at improving and correcting the program in the areas where such issues occur."
